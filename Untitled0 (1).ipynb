{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"WYjXBRYGWO5g","colab_type":"text"},"cell_type":"markdown","source":["# **Predicting the Category**\n","\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"T9kowNLpXIK6","colab_type":"text"},"cell_type":"markdown","source":["we need to import the following module "]},{"metadata":{"id":"20r0Sq9LXHCv","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"code"},"cell_type":"code","source":["# pip install scikit learn\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.datasets import fetch_20newsgroups\n","#import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gzt-3a3pXnj3","colab_type":"text"},"cell_type":"markdown","source":["install the Scikit learn module using pip\n","\n","\n","1.   in this we will be classifying using naive_bayes \n","2.   importing 20 news groups dataset from sklearn \n","\n","\n"]},{"metadata":{"id":"-7eZIQUWZhZW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["categories = ['alt.atheism', 'soc.religion.christian','comp.graphics','sci.med'] #listing the categories\n","twenty_train = fetch_20newsgroups(subset='train',categories=categories,shuffle=True,random_state=42) #fetching the dataset\n","print(twenty_train.target_names)\n","print(len(twenty_train.data))\n","print(len(twenty_train.filenames))\n","print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n","print(twenty_train.target[:10])\n","#plt.scatter(twenty_train.target[:10],range(10))\n","#plt.show()\n","\n","for t in twenty_train.target[:10]:\n","    print(twenty_train.target_names[t])\n","\n","count_vect = CountVectorizer() \n","X_train_count = count_vect.fit_transform(twenty_train.data) #need to convert it into word\n","print(X_train_count.shape)\n","print(count_vect.vocabulary_.get(u'algorithm'))\n","tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_count) #fitting the count \n","X_train_tf = tf_transformer.transform(X_train_count)\n","print(X_train_tf.shape)\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n","print(X_train_tfidf.shape)\n","\n","clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target) #naive_bayes\n","docs_new = ['Google Graphic' , 'fast CPU']\n","X_new_counts = count_vect.transform(docs_new)\n","X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n","pred = clf.predict(X_new_tfidf) \n","for doc ,category in zip(docs_new,pred): \n","    print('{} {}' .format(doc,twenty_train.target_names[category]))  #predcting the category\n"],"execution_count":0,"outputs":[]}]}